
# Quiet Crawler Project Structure

This repository supports the Claude API-based incident analysis workflow. It includes scraper outputs, structured source data, codebook files, and utility scripts, all organized for clarity, auditability, and maintainability.

---

## ğŸ“‚ Directory Structure

```
.
quiet-crawler/
â”‚
â”œâ”€â”€ inputs/                             # Claude API prompt inputs (modular, editable)
â”‚   â”œâ”€â”€ system/                             # System-level configuration (shared across all incidents)
â”‚   â”‚   â”œâ”€â”€ system_role.txt                   # Claude's persona (e.g., "You are an evidence auditor...")
â”‚   â”‚   â”œâ”€â”€ justification_protocol.txt        # How to extract and attribute quotes and output justification block (YAML format)
â”‚   â”‚   â”œâ”€â”€ definitions.txt                   # Incident boundaries, source types, codebook meta-rules
â”‚   â”‚   â”œâ”€â”€ codebook.md                       # Variable dictionary with descriptions and values
â”‚   â”‚   â”œâ”€â”€ codebook_protocol.md              # Logic for applying codebook (e.g., disqualifying evidence checks)
â”‚   â”‚   â””â”€â”€ verifications.txt                 # Claude must confirm checklist (e.g. "I have reviewed all sources")
â”‚   â”œâ”€â”€ incident/                           # Per-incident user message inputs
â”‚   â”‚   â”œâ”€â”€ incident_description.txt          # Summary + metadata (incident_id, date, scope)
â”‚   â”‚   â”œâ”€â”€ user_prompt.txt                   # Task framing for Claude ("Here is an incident I want you to code...")
â”‚   â”‚   â”œâ”€â”€ audit_instructions.txt            # Central command points to modules, then requests COT, verification and justification
â”‚
â”œâ”€â”€ sources/                            # Final cleaned source text files (used for incident coding)
â”‚   â”œâ”€â”€ ADM-001.txt
â”‚   â”œâ”€â”€ DB-001.txt                       # DB sources are a chosen subset (by incident criteria) of scraped articles
â”‚   â”œâ”€â”€ SOC-001.txt                      # Each SOC is one reddit/IG/Twitter post (images are post-processed to txt)
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ SOC-001.png                   # Pre-processed image source files
â”‚
â”œâ”€â”€ _data/                              # Manually curated metadata for writing + visualization
â”‚   â”œâ”€â”€ codebook.md                       # Public-facing variable definitions
â”‚   â”œâ”€â”€ codebook_w_coding_proto_v2.md     # Codebook with integrated Claude-specific logic
â”‚   â”œâ”€â”€ source_master.yml                 # Master list of all sources with tags + file mapping
â”‚   â”œâ”€â”€ variable_data.yml                 # Structured schema for all coding variables
â”‚   â””â”€â”€ incident_data.yml                 # Central list of all incident records
â”‚
â”œâ”€â”€ scraper_inputs/                     # Scraping input config
â”‚   â”œâ”€â”€ daily_bruin/
â”‚   â”‚   â”œâ”€â”€ universal_keywords.txt
â”‚   â”‚   â””â”€â”€ universal_incident_rule.md
â”‚   â””â”€â”€ reddit/
â”‚       â””â”€â”€ INC-001-reddit-urls.json
â”‚
â”œâ”€â”€ scraper_outputs/                    # Raw scraped data before cleaning
â”‚   â”œâ”€â”€ raw_text/
â”‚   â”‚   â”œâ”€â”€ DB-raw-001.txt              # All scraped DB articles
â”‚   â”‚   â””â”€â”€ INC-001_reddit-scraped.txt  # Flattened, multiple posts in one file
â”‚   â””â”€â”€ json/
â”‚       â”œâ”€â”€ DB-raw-001.json             
â”‚       â””â”€â”€ INC-001_reddit_scraped.json # Dicts with metadata and comments
â”‚
â”œâ”€â”€ scrapers/                           # Web scraping logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_base.py
â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â””â”€â”€ daily_bruin_scraper.py
â”‚
â”œâ”€â”€ utils/                              # Helper scripts and converters
â”‚   â”œâ”€â”€ doc_loader.py                     # Assembles (some, all?) Claude API messages from input folders
â”‚   â”œâ”€â”€ reddit_json_to_txt_converter.py   # Flattens scraped Reddit data into clean .txt
â”‚   â””â”€â”€ txt_to_yaml_converter.py          # Converts Claude text output to structured YAML
â”‚
â”œâ”€â”€ api_tests/                          # Claude API test runners
â”‚   â””â”€â”€ test_claude.py                    # Core call logic, logging, model selection
â”‚
â”œâ”€â”€ outputs/                            # Claude outputs (per-incident)
â”‚   â”œâ”€â”€ audit_log/                        # Full Claude response + header
â”‚   â”‚   â””â”€â”€ INC-001-audit-log.txt
â”‚   â”œâ”€â”€ coded_text/                       # Claude's structured YAML-like response (raw .txt)
â”‚   â”‚   â””â”€â”€ INC-001-coded-output.txt
â”‚   â””â”€â”€ coded_output/                     # Final YAML after validation/parsing
â”‚       â””â”€â”€ INC-001-coded-output.yml
â”‚
â”œâ”€â”€ main.py                             # Entry point for incident processing pipeline
â”œâ”€â”€ README.md                           # Project overview + file usage
â””â”€â”€ venv/                               # Python virtual environment

```
---

## Key Folder Notes

- **sources/** â€” Contains the finalized .txt files that Claude will cite. Each file matches a source ID defined in `source_master.yml`.
- **_data/** â€” Contains structured metadata:
  - `source_master.yml` â€” Maps source IDs (e.g. ADM-001) to filenames and descriptions.
  - `field_definitions.yml` â€” Defines variables, types, and valid values for consistency and validation.
- **scraper_outputs/** â€” Raw outputs from the web scraper:
  - `raw_text/` â€” Unprocessed text dumps for debugging and reprocessing.
  - `json/` â€” Structured JSON outputs from scrapers, useful for transforming into .txt files.
- **scrapers/** â€” Dedicated Python modules for web scraping tasks:
  - Keeps scraper code organized by source (e.g. Daily Bruin, social media).
  - Allows easy maintenance and extension of scraping logic.
- **utils/** â€” Python scripts that support reading sources, metadata, and transforming scraper outputs:
  - `source_reader.py` â€” Loads text sources and metadata into memory.
  - `json_to_txt_converter.py` â€” Converts JSON outputs into clean .txt files that Claude can process.
- **api_tests/** â€” Contains test scripts to verify Claude API integration with sample prompts and documents.
- **main.py** â€” The main script that orchestrates the full pipeline, including processing incidents in batch and coordinating Claude API calls.
- **venv/** â€” Your isolated Python environment to install dependencies without polluting the system environment.

---

## Scraping criteria

### Reddit

It is a 10-5-5 model now, update. 

```
.
ğŸ“„ Post (submission)
  â”œâ”€â”€ ğŸ’¬ Top-Level Comment 1
  â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.1
  â”‚     â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.1.1
  â”‚     â”‚     â””â”€â”€ â†ªï¸ Reply 1.1.2
  â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.2
  â”‚     â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.2.1
  â”‚     â”‚     â””â”€â”€ â†ªï¸ Reply 1.2.2
  â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.3
  â”‚     â”‚     â”œâ”€â”€ â†ªï¸ Reply 1.3.1
  â”‚     â”‚     â””â”€â”€ â†ªï¸ Reply 1.3.2
  â”‚     â””â”€â”€ â†ªï¸ Reply 1.4
  â”‚           â”œâ”€â”€ â†ªï¸ Reply 1.4.1
  â”‚           â””â”€â”€ â†ªï¸ Reply 1.4.2
  â”‚
  â”œâ”€â”€ ğŸ’¬ Top-Level Comment 2
  â”‚     â”œâ”€â”€ â†ªï¸ Reply 2.1
  â”‚     â”‚     â”œâ”€â”€ â†ªï¸ Reply 2.1.1
  â”‚     â”‚     â””â”€â”€ â†ªï¸ Reply 2.1.2
  â”‚     â””â”€â”€ ... (repeats similar structure as above)
  â”‚
  â””â”€â”€ ğŸ’¬ Top-Level Comment 3-10
        â””â”€â”€ ... (same nested structure as above)

```  
---

## Workflow

1. **Scrape Data** â€” Use the scrapers in `scrapers/` to collect raw data.
2. **Convert Outputs** â€” Transform JSON or raw text into cleaned `.txt` files in `sources/`.
3. **Organize Metadata** â€” Maintain `source_master.yml` and `field_definitions.yml` in `_data/`.
4. **Run Tests** â€” Use `api_tests/test_claude.py` to test Claude prompts and validate output structure.
5. **Full Pipeline** â€” Execute `main.py` to process incidents in batch, enforce coding protocols, and generate structured YAML outputs.

---

## Tips

- Keep `sources/` clean and consistentâ€”one file per source ID, always matching `source_master.yml`.
- Keep `scraper_outputs/` intact as your raw recordâ€”donâ€™t edit files there directly.
- Regularly update `variable_data.yml` and `source_master.yml` to stay aligned with your evolving coding protocols.

---
