
# Quiet Crawler Project Structure

This repository supports the Claude API-based incident analysis workflow. It includes scraper outputs, structured source data, codebook files, and utility scripts, all organized for clarity, auditability, and maintainability.

---

## ðŸ“‚ Directory Structure

```
.
quiet-crawler/
â”‚
â”œâ”€â”€ sources/ # Final cleaned source text files, source numbering is independent from incident numbering
â”‚   â”œâ”€â”€ ADM-001.txt
â”‚   â”œâ”€â”€ DB-001.txt  # DB sources are a subset (by incident criteria) of raw outputs
â”‚   â”œâ”€â”€ SOC-001.txt # Each SOC holds one reddit/IG/Twitter post 
â”‚
â”œâ”€â”€ _data/ # Manually curated markdown and YAML data files
â”‚   â”œâ”€â”€ codebook.md
â”‚   â”œâ”€â”€ codebook_w_coding_proto_v2.md
â”‚   â”œâ”€â”€ source_master.yml
â”‚   â”œâ”€â”€ field_definitions.yml
â”‚
â”œâ”€â”€ scraper_inputs/
â”‚   â”œâ”€â”€ daily_bruin/
â”‚   â”‚   â”œâ”€â”€ universal_keywords.txt
â”‚   â”‚   â””â”€â”€ universal_incident_rule.md
â”‚   â””â”€â”€ reddit/
â”‚       â”œâ”€â”€ INC-001-reddit-urls.json
â”‚       â”œâ”€â”€ INC-001-reddit-urls.json
â”‚
â”œâ”€â”€ scraper_outputs/ # Raw outputs from the web scraper 
â”‚   â”œâ”€â”€ raw_text/
â”‚   â”‚   â”œâ”€â”€ DB-raw-001.txt
â”‚   â”‚   â”œâ”€â”€ INC-001_reddit-raw-001.txt # Flattened but still multiple posts in one file
â”‚   â”œâ”€â”€ json/
â”‚   â”‚   â”œâ”€â”€ DB-raw-001.json
â”‚   â”‚   â”œâ”€â”€ INC-001_reddit_scaped.json # Dicts with metadata and comments
â”‚
â”œâ”€â”€ scrapers/ # Web scraping code modules
â”‚   â”œâ”€â”€ init.py
â”‚   â”œâ”€â”€ scraper_base.py
â”‚   â”œâ”€â”€ reddit_scraper.py  
â”‚   â”œâ”€â”€ daily_bruin_scraper.py
â”‚
â”œâ”€â”€ utils/ # Helper Python scripts
â”‚   â”œâ”€â”€ source_reader.py
â”‚   â”œâ”€â”€ json_to_txt_converter.py
â”‚
â”œâ”€â”€ api_tests/ # API test scripts for Claude integration
â”‚   â””â”€â”€ test_claude.py
â”‚
â”œâ”€â”€ outputs/ # Claude audit logs and YAML outputs per incident
â”‚   â”œâ”€â”€ INC-001/
â”‚   â”‚   â”œâ”€â”€ audit-log.txt
â”‚   â”‚   â””â”€â”€ coded-output.yml
â”‚   â”œâ”€â”€ INC-002/
â”‚   â”‚   â”œâ”€â”€ audit-log.txt
â”‚   â”‚   â””â”€â”€ coded-output.yml
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ main.py # Entry point for running the full incident analysis pipeline
â”œâ”€â”€ README.md # Project overview and directory structure
â””â”€â”€ venv/ # Python virtual environment (created via python -m venv)

```
---

## Key Folder Notes

- **sources/** â€” Contains the finalized .txt files that Claude will cite. Each file matches a source ID defined in `source_master.yml`.
- **_data/** â€” Contains structured metadata:
  - `source_master.yml` â€” Maps source IDs (e.g. ADM-001) to filenames and descriptions.
  - `field_definitions.yml` â€” Defines variables, types, and valid values for consistency and validation.
- **scraper_outputs/** â€” Raw outputs from the web scraper:
  - `raw_text/` â€” Unprocessed text dumps for debugging and reprocessing.
  - `json/` â€” Structured JSON outputs from scrapers, useful for transforming into .txt files.
- **scrapers/** â€” Dedicated Python modules for web scraping tasks:
  - Keeps scraper code organized by source (e.g. Daily Bruin, social media).
  - Allows easy maintenance and extension of scraping logic.
- **utils/** â€” Python scripts that support reading sources, metadata, and transforming scraper outputs:
  - `source_reader.py` â€” Loads text sources and metadata into memory.
  - `json_to_txt_converter.py` â€” Converts JSON outputs into clean .txt files that Claude can process.
- **api_tests/** â€” Contains test scripts to verify Claude API integration with sample prompts and documents.
- **main.py** â€” The main script that orchestrates the full pipeline, including processing incidents in batch and coordinating Claude API calls.
- **venv/** â€” Your isolated Python environment to install dependencies without polluting the system environment.

---

## Workflow

1. **Scrape Data** â€” Use the scrapers in `scrapers/` to collect raw data.
2. **Convert Outputs** â€” Transform JSON or raw text into cleaned `.txt` files in `sources/`.
3. **Organize Metadata** â€” Maintain `source_master.yml` and `field_definitions.yml` in `_data/`.
4. **Run Tests** â€” Use `api_tests/test_claude.py` to test Claude prompts and validate output structure.
5. **Full Pipeline** â€” Execute `main.py` to process incidents in batch, enforce coding protocols, and generate structured YAML outputs.

---

## Tips

- Keep `sources/` clean and consistentâ€”one file per source ID, always matching `source_master.yml`.
- Keep `scraper_outputs/` intact as your raw recordâ€”donâ€™t edit files there directly.
- Regularly update `field_definitions.yml` and `source_master.yml` to stay aligned with your evolving coding protocols.

---
